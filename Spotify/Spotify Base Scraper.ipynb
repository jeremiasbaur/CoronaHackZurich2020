{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes_html = requests.get('https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2')\n",
    "country_codes_df = pd.read_html(country_codes_html.text, flavor ='html5lib', header = 0)[2]\n",
    "country_code_list = list(country_codes_df['Country name (using title case)'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code_list[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_charts = requests.get('https://spotifycharts.com/regional')\n",
    "spotify_soup = BeautifulSoup(spotify_charts.text, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_countries = []\n",
    "\n",
    "for i in spotify_soup.find_all('li'):\n",
    "    \n",
    "    if i.text in country_code_list:\n",
    "        \n",
    "        spotify_countries.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = country_codes_df[country_codes_df['Country name (using title case)'].isin(spotify_countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_country_codes = list(sub_df['Code'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_country_codes[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = []\n",
    "\n",
    "for i in spotify_soup.find_all('li'):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        date = pd.to_datetime(i.text)\n",
    "        date = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        date = np.NaN\n",
    "        \n",
    "    date_list.append(date)\n",
    "    \n",
    "date_list_clean = [x for x in date_list if x != 'nan' and type(x) != float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list_clean\n",
    "len(date_list_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CH']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_country_codes[8:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data available for CH, 2020-09-17\n",
      "Data available for CH, 2020-09-16\n",
      "Data available for CH, 2020-09-15\n",
      "Data available for CH, 2020-09-14\n",
      "Data available for CH, 2020-09-13\n",
      "Data available for CH, 2020-09-12\n",
      "Data available for CH, 2020-09-11\n",
      "Data available for CH, 2020-09-10\n",
      "Data available for CH, 2020-09-09\n",
      "Data available for CH, 2020-09-08\n",
      "Data available for CH, 2020-09-07\n",
      "Data available for CH, 2020-09-06\n",
      "Data available for CH, 2020-09-05\n",
      "Data available for CH, 2020-09-04\n",
      "Data available for CH, 2020-09-03\n",
      "Data available for CH, 2020-09-02\n",
      "Data available for CH, 2020-09-01\n",
      "Data available for CH, 2020-08-31\n",
      "Data available for CH, 2020-08-30\n",
      "Data available for CH, 2020-08-29\n",
      "Data available for CH, 2020-08-28\n",
      "Data available for CH, 2020-08-27\n",
      "Data available for CH, 2020-08-26\n",
      "Data available for CH, 2020-08-25\n",
      "Data available for CH, 2020-08-24\n",
      "Data available for CH, 2020-08-23\n",
      "Data available for CH, 2020-08-22\n",
      "Data available for CH, 2020-08-21\n",
      "Data available for CH, 2020-08-20\n",
      "Data available for CH, 2020-08-19\n",
      "Data available for CH, 2020-08-18\n",
      "Data available for CH, 2020-08-17\n",
      "Data available for CH, 2020-08-16\n",
      "Data available for CH, 2020-08-15\n",
      "Data available for CH, 2020-08-14\n",
      "Data available for CH, 2020-08-13\n",
      "Data available for CH, 2020-08-12\n",
      "Data available for CH, 2020-08-11\n",
      "Data available for CH, 2020-08-10\n",
      "Data available for CH, 2020-08-09\n",
      "Data available for CH, 2020-08-08\n",
      "Data available for CH, 2020-08-07\n",
      "Data available for CH, 2020-08-06\n",
      "Data available for CH, 2020-08-05\n",
      "Data available for CH, 2020-08-04\n",
      "Data available for CH, 2020-08-03\n",
      "Data available for CH, 2020-08-02\n",
      "Data available for CH, 2020-08-01\n",
      "Data available for CH, 2020-07-31\n",
      "Data available for CH, 2020-07-30\n",
      "Data available for CH, 2020-07-29\n",
      "Data available for CH, 2020-07-28\n",
      "Data available for CH, 2020-07-27\n",
      "Data available for CH, 2020-07-26\n",
      "Data available for CH, 2020-07-25\n",
      "Data available for CH, 2020-07-24\n",
      "Data available for CH, 2020-07-23\n",
      "Data available for CH, 2020-07-22\n",
      "Data available for CH, 2020-07-21\n",
      "Data available for CH, 2020-07-20\n",
      "Data available for CH, 2020-07-19\n",
      "Data available for CH, 2020-07-18\n",
      "Data available for CH, 2020-07-17\n",
      "Data available for CH, 2020-07-16\n",
      "Data available for CH, 2020-07-15\n",
      "Data available for CH, 2020-07-14\n",
      "Data available for CH, 2020-07-13\n",
      "Data available for CH, 2020-07-12\n",
      "Data available for CH, 2020-07-11\n",
      "Data available for CH, 2020-07-10\n",
      "Data available for CH, 2020-07-09\n",
      "Data available for CH, 2020-07-08\n",
      "Data available for CH, 2020-07-07\n",
      "Data available for CH, 2020-07-06\n",
      "Data available for CH, 2020-07-05\n",
      "Data available for CH, 2020-07-04\n",
      "Data available for CH, 2020-07-03\n",
      "Data available for CH, 2020-07-02\n",
      "Data available for CH, 2020-07-01\n",
      "Data available for CH, 2020-06-30\n",
      "Data available for CH, 2020-06-29\n",
      "Data available for CH, 2020-06-28\n",
      "Data available for CH, 2020-06-27\n",
      "Data available for CH, 2020-06-26\n",
      "Data available for CH, 2020-06-25\n",
      "Data available for CH, 2020-06-24\n",
      "Data available for CH, 2020-06-23\n",
      "Data available for CH, 2020-06-22\n",
      "Data available for CH, 2020-06-21\n",
      "Data available for CH, 2020-06-20\n",
      "Data available for CH, 2020-06-19\n",
      "Data available for CH, 2020-06-18\n",
      "Data available for CH, 2020-06-17\n",
      "Data available for CH, 2020-06-16\n",
      "Data available for CH, 2020-06-15\n",
      "Data available for CH, 2020-06-14\n",
      "Data available for CH, 2020-06-13\n",
      "Data available for CH, 2020-06-12\n",
      "Data available for CH, 2020-06-11\n",
      "Data available for CH, 2020-06-10\n",
      "Data available for CH, 2020-06-09\n",
      "Data available for CH, 2020-06-08\n",
      "Data available for CH, 2020-06-07\n",
      "Data available for CH, 2020-06-06\n",
      "Data available for CH, 2020-06-05\n",
      "Data available for CH, 2020-06-04\n",
      "Data available for CH, 2020-06-03\n",
      "Data available for CH, 2020-06-02\n",
      "Data available for CH, 2020-06-01\n",
      "Data available for CH, 2020-05-31\n",
      "Data available for CH, 2020-05-30\n",
      "Data available for CH, 2020-05-29\n",
      "Data available for CH, 2020-05-28\n",
      "Data available for CH, 2020-05-27\n",
      "Data available for CH, 2020-05-26\n",
      "Data available for CH, 2020-05-25\n",
      "Data available for CH, 2020-05-24\n",
      "Data available for CH, 2020-05-23\n",
      "Data available for CH, 2020-05-22\n",
      "Data available for CH, 2020-05-21\n",
      "Data available for CH, 2020-05-20\n",
      "Data available for CH, 2020-05-19\n",
      "Data available for CH, 2020-05-18\n",
      "Data available for CH, 2020-05-17\n",
      "Data available for CH, 2020-05-16\n",
      "Data available for CH, 2020-05-15\n",
      "Data available for CH, 2020-05-14\n",
      "Data available for CH, 2020-05-13\n",
      "Data available for CH, 2020-05-12\n",
      "Data available for CH, 2020-05-11\n",
      "Data available for CH, 2020-05-10\n",
      "Data available for CH, 2020-05-09\n",
      "Data available for CH, 2020-05-08\n",
      "Data available for CH, 2020-05-07\n",
      "Data available for CH, 2020-05-06\n",
      "Data available for CH, 2020-05-05\n",
      "Data available for CH, 2020-05-04\n",
      "Data available for CH, 2020-05-03\n",
      "Data available for CH, 2020-05-02\n",
      "Data available for CH, 2020-05-01\n",
      "Data available for CH, 2020-04-30\n",
      "Data available for CH, 2020-04-29\n",
      "Data available for CH, 2020-04-28\n",
      "Data available for CH, 2020-04-27\n",
      "Data available for CH, 2020-04-26\n",
      "Data available for CH, 2020-04-25\n",
      "Data available for CH, 2020-04-24\n",
      "Data available for CH, 2020-04-23\n",
      "Data available for CH, 2020-04-22\n",
      "Data available for CH, 2020-04-21\n",
      "Data available for CH, 2020-04-20\n",
      "Data available for CH, 2020-04-19\n",
      "Data available for CH, 2020-04-18\n",
      "Data available for CH, 2020-04-17\n",
      "Data available for CH, 2020-04-16\n",
      "Data available for CH, 2020-04-15\n",
      "Data available for CH, 2020-04-14\n",
      "Data available for CH, 2020-04-13\n",
      "Data available for CH, 2020-04-12\n",
      "Data available for CH, 2020-04-11\n",
      "Data available for CH, 2020-04-10\n",
      "Data available for CH, 2020-04-09\n",
      "Data available for CH, 2020-04-08\n",
      "Data available for CH, 2020-04-07\n",
      "Data available for CH, 2020-04-06\n",
      "Data available for CH, 2020-04-05\n",
      "Data available for CH, 2020-04-04\n",
      "Data available for CH, 2020-04-03\n",
      "Data available for CH, 2020-04-02\n",
      "Data available for CH, 2020-04-01\n",
      "Data available for CH, 2020-03-31\n",
      "Data available for CH, 2020-03-30\n",
      "Data available for CH, 2020-03-29\n",
      "Data available for CH, 2020-03-28\n",
      "Data available for CH, 2020-03-27\n",
      "Data available for CH, 2020-03-26\n",
      "Data available for CH, 2020-03-25\n",
      "Data available for CH, 2020-03-24\n",
      "Data available for CH, 2020-03-23\n",
      "Data available for CH, 2020-03-22\n",
      "Data available for CH, 2020-03-21\n",
      "Data available for CH, 2020-03-20\n",
      "Data available for CH, 2020-03-19\n",
      "Data available for CH, 2020-03-18\n",
      "Data available for CH, 2020-03-17\n",
      "Data available for CH, 2020-03-16\n",
      "Data available for CH, 2020-03-15\n",
      "Data available for CH, 2020-03-14\n",
      "Data available for CH, 2020-03-13\n",
      "Data available for CH, 2020-03-12\n",
      "Data available for CH, 2020-03-11\n",
      "Data available for CH, 2020-03-10\n",
      "Data available for CH, 2020-03-09\n",
      "Data available for CH, 2020-03-08\n",
      "Data available for CH, 2020-03-07\n",
      "Data available for CH, 2020-03-06\n",
      "Data available for CH, 2020-03-05\n",
      "Data available for CH, 2020-03-04\n",
      "Data available for CH, 2020-03-03\n",
      "Data available for CH, 2020-03-02\n",
      "Data available for CH, 2020-03-01\n",
      "Data available for CH, 2020-02-29\n",
      "Data available for CH, 2020-02-28\n",
      "Data available for CH, 2020-02-27\n",
      "Data available for CH, 2020-02-26\n",
      "Data available for CH, 2020-02-25\n",
      "Data available for CH, 2020-02-24\n",
      "Data available for CH, 2020-02-23\n",
      "Data available for CH, 2020-02-22\n",
      "Data available for CH, 2020-02-21\n",
      "Data available for CH, 2020-02-20\n",
      "Data available for CH, 2020-02-19\n",
      "Data available for CH, 2020-02-18\n",
      "Data available for CH, 2020-02-17\n",
      "Data available for CH, 2020-02-16\n",
      "Data available for CH, 2020-02-15\n",
      "Data available for CH, 2020-02-14\n",
      "Data available for CH, 2020-02-13\n",
      "Data available for CH, 2020-02-12\n",
      "Data available for CH, 2020-02-11\n",
      "Data available for CH, 2020-02-10\n",
      "Data available for CH, 2020-02-09\n",
      "Data available for CH, 2020-02-08\n",
      "Data available for CH, 2020-02-07\n",
      "Data available for CH, 2020-02-06\n",
      "Data available for CH, 2020-02-05\n",
      "Data available for CH, 2020-02-04\n",
      "Data available for CH, 2020-02-03\n",
      "Data available for CH, 2020-02-02\n",
      "Data available for CH, 2020-02-01\n",
      "Data available for CH, 2020-01-31\n",
      "Data available for CH, 2020-01-30\n",
      "Data available for CH, 2020-01-29\n",
      "Data available for CH, 2020-01-28\n",
      "Data available for CH, 2020-01-27\n",
      "Data available for CH, 2020-01-26\n",
      "Data available for CH, 2020-01-25\n",
      "Data available for CH, 2020-01-24\n",
      "Data available for CH, 2020-01-23\n",
      "Data available for CH, 2020-01-22\n",
      "Data available for CH, 2020-01-21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data available for CH, 2020-01-20\n",
      "Data available for CH, 2020-01-19\n",
      "Data available for CH, 2020-01-18\n",
      "Data available for CH, 2020-01-17\n",
      "Data available for CH, 2020-01-16\n",
      "Data available for CH, 2020-01-15\n",
      "Data available for CH, 2020-01-14\n",
      "Data available for CH, 2020-01-13\n",
      "Data available for CH, 2020-01-12\n",
      "Data available for CH, 2020-01-11\n",
      "Data available for CH, 2020-01-10\n",
      "Data available for CH, 2020-01-09\n",
      "Data available for CH, 2020-01-08\n",
      "Data available for CH, 2020-01-07\n",
      "Data available for CH, 2020-01-06\n",
      "Data available for CH, 2020-01-05\n",
      "Data available for CH, 2020-01-04\n",
      "Data available for CH, 2020-01-03\n",
      "Data available for CH, 2020-01-02\n",
      "Data available for CH, 2020-01-01\n",
      "Data retrieval finished!\n"
     ]
    }
   ],
   "source": [
    "seed_url = 'https://spotifycharts.com/regional/'\n",
    "frequency = 'daily'\n",
    "\n",
    "country_df_list = []\n",
    "\n",
    "for country in spotify_country_codes[8:9]:\n",
    "    \n",
    "    date_df_list = []\n",
    "    \n",
    "    for date in date_list_clean[:261]:\n",
    "        \n",
    "        url = seed_url + country.lower() + '/' + frequency + '/' + date # I concatenate the base url with the variable components\n",
    "                                \n",
    "        response = requests.get(url) # and send a request for every new URL\n",
    "        \n",
    "        if response.status_code == 200: # this line checks whether the response from the request was positive (i.e. code 200)\n",
    "            \n",
    "            print(\"Data available for \" + country + \", \" + date)\n",
    "            \n",
    "            soup_response = BeautifulSoup(response.text, \"html5lib\")\n",
    "            response_links = soup_response.find_all('a')\n",
    "            \n",
    "            track_links = []\n",
    "            track_ids = []\n",
    "\n",
    "            for link in response_links:\n",
    "    \n",
    "                if type(link.get('href')) == str and 'track' in link.get('href'):\n",
    "            \n",
    "                # this condition checks whether the elements contained in the list 'response_links' which were identified\n",
    "                # by the selector 'a' are 1) of type string and 2) the link property contains the substring 'track'\n",
    "                # since I only want entries from the charts website which have a valid track URL\n",
    "                # Note: there were cases where this wasn't the case, i.e. empty rows or rows without a track URL\n",
    "                # which led to inconsistencies when merging all dataframes at the end\n",
    "            \n",
    "                    track_link = link.get('href')\n",
    "                    track_id = link.get('href').split('/')[-1]\n",
    "        \n",
    "                    track_links.append(track_link)\n",
    "                    track_ids.append(track_id)\n",
    "                             \n",
    "            df = pd.read_html(response.text, flavor ='html5lib', header= 0)[0] # This retrieves the whole table as dataframe\n",
    "            \n",
    "            column_names = ['0', 'Rank', '1', 'Title_Artist', 'Streams'] # I assign column names\n",
    "            df.columns = column_names # and apply the new names to the columns of the dataframe 'df'\n",
    "            \n",
    "            na_index = pd.notna(df['Title_Artist']) # I keep only rows where the 'Title_Artist' column is different from\n",
    "                                                    # nan\n",
    "            \n",
    "            na_index_num = [i for i, x in enumerate(na_index) if x]\n",
    "            \n",
    "            df = df[pd.notna(df['Title_Artist'])]\n",
    "            \n",
    "            # The condition below is necessary whenever there was an invalid (i.e. missing) Track-URL in the table \n",
    "            # but the table contained other columns for the associated row.\n",
    "            \n",
    "            if len(track_links) != len(df):\n",
    "                \n",
    "                df['Track_URL'] = list(np.array(track_links)[na_index_num])\n",
    "                df['Track_ID'] = list(np.array(track_ids)[na_index_num])\n",
    "                \n",
    "                # Now the length of the 'df' and valid 'track_links' indices do match and can be assigned as columns.            \n",
    "            \n",
    "            else: \n",
    "                \n",
    "                df['Track_URL'] = track_links\n",
    "                df['Track_ID'] = track_ids\n",
    "                \n",
    "                # For this case no exceptions were needed as the initial table contained as many valid Track-Urls as number \n",
    "                # of rows.\n",
    "            \n",
    "            df = df[['Rank', 'Title_Artist', 'Streams', 'Track_URL', 'Track_ID']]\n",
    "            \n",
    "            # I keep the columns 'Rank', 'Title_Artist', 'Streams', 'Track_URL' and 'Track_ID'\n",
    "                 \n",
    "            title_list = []\n",
    "            artist_list = []\n",
    "            \n",
    "            # The loop below splits the concatenated column 'Title_Artist' into 'Title' and 'Artist' at the separator ' by '\n",
    "\n",
    "            for element in list(df['Title_Artist'].values):\n",
    "                \n",
    "                if type(element) == str:\n",
    "    \n",
    "                    title = element.split(\" by \")[0]\n",
    "                    artist = element.split(\" by \")[1]\n",
    "    \n",
    "                    title_list.append(title)\n",
    "                    artist_list.append(artist)\n",
    "            \n",
    "                else:\n",
    "                    \n",
    "                    title = np.NaN\n",
    "                    artist = np.NaN\n",
    "                    \n",
    "                    title_list.append(title)\n",
    "                    artist_list.append(artist)\n",
    "            \n",
    "            df['Track title'] = title_list\n",
    "            df['Artist'] = artist_list\n",
    "            df['country_code'] = country\n",
    "            df['date'] = date\n",
    "            \n",
    "            date_df_list.append(df)\n",
    "            \n",
    "            # Finally the cleaned, resulting daily charts by country dataframe is appended to a list 'date_df_list'\n",
    "            \n",
    "            # It is good practice not to send request after request. Depending on the time of execution your program could\n",
    "            # send too many queries within a period of time and the server could block your IP. \n",
    "            # Therefore, include a break of one second by 'time.sleep(1)'\n",
    "            \n",
    "#             time.sleep(0.5)\n",
    "            \n",
    "        elif response.status_code == 404:\n",
    "            \n",
    "            print(\"No data available for \" + country + \", \" + date)\n",
    "            \n",
    "            date_df_list = []\n",
    "            \n",
    "        # The condition below checks whether the 'date_df_list' by country has at least one entry (i.e. day) and if it's true\n",
    "        # it merges the contained dataframes into one big dataframe and saves it as .csv to your directory.\n",
    "        # Note that in every iteration the previous table is appended and the previously saved .csv is overwritten to save\n",
    "        # storage.\n",
    "        \n",
    "        if len(date_df_list) > 0:\n",
    "            \n",
    "            date_df_merged = pd.concat(date_df_list)\n",
    "            \n",
    "#             date_df_merged.to_csv(str(date) + \"_\" + country + \".csv\")\n",
    "\n",
    "            date_df_merged.to_csv(\"Latest_\" + country + \".csv\")\n",
    "    \n",
    "    # The line below merges all saved country dataframes into one big dataframe containing daily charts across available\n",
    "    # countries.\n",
    "    \n",
    "    country_df_list.append(date_df_merged)\n",
    "    \n",
    "print('Data retrieval finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
